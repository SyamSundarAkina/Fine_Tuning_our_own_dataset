# ğŸ¤– LoRA Fine-Tuning with GPT-2 for Emotion Classification

This project fine-tunes a GPT-2 model using **LoRA (Low-Rank Adaptation)** for **emotion classification**, enabling efficient training with fewer parameters. It uses the Hugging Face Transformers and PEFT libraries, and evaluates the model using BLEU, ROUGE, and METEOR metrics.

---

## ğŸ“Œ Features

- âœ… Fine-tunes GPT-2 using LoRA for reduced compute and memory usage
- ğŸ§  Emotion classification from user text inputs
- ğŸ§¹ Cleans and converts dataset into JSONL format for instruction-tuned training
- ğŸ“‰ Evaluates performance using BLEU, ROUGE, and METEOR
- ğŸ’¾ Saves fine-tuned model for inference or future use

---

## ğŸ› ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/SyamSundarAkina/Fine_Tuning_our_own_dataset.git
cd Fine_Tuning_our_own_dataset

