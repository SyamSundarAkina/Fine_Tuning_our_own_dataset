# Fine_Tuning_our_own_dataset
This project fine-tunes GPT-2 using LoRA for emotion classification, preprocessing data into JSONL, training efficiently with Hugging Face Transformers and PEFT (8-bit quantization), and evaluating performance via BLEU, ROUGE, and METEOR metrics. Final model outputs are saved for inference and validation.
