# ğŸ¤– LoRA Fine-Tuning with GPT-2 for Emotion Classification

This project fine-tunes a GPT-2 model using **LoRA (Low-Rank Adaptation)** for **emotion classification**, enabling efficient training with fewer parameters. It uses the Hugging Face Transformers and PEFT libraries, and evaluates the model using BLEU, ROUGE, and METEOR metrics.

---

## ğŸ“Œ Features

- âœ… Fine-tunes GPT-2 using LoRA for reduced compute and memory usage  
- ğŸ§  Emotion classification from user text inputs  
- ğŸ§¹ Cleans and converts dataset into JSONL format for instruction-tuned training  
- ğŸ“‰ Evaluates performance using BLEU, ROUGE, and METEOR  
- ğŸ’¾ Saves fine-tuned model for inference or future use  

---

## ğŸ’¡ Use Cases

This fine-tuned model can be applied in the following scenarios:

- **Customer Support** â€“ Detects emotional tone in messages to improve service response and escalation.  
- **Chatbots** â€“ Enables emotion-aware virtual assistants that adapt tone and replies.  
- **Mental Health Applications** â€“ Monitors user sentiment in journals or messages for emotional well-being tracking.  
- **Social Media Analysis** â€“ Identifies emotion trends in tweets, posts, and reviews for brand or public sentiment.  
- **EdTech and HR Platforms** â€“ Analyzes feedback and engagement sentiment to enhance learning or workplace experience.

---

## ğŸ“ˆ Fine-Tuning Workflow

<img src="./assets/workflow.png" alt="Workflow Diagram" width="300"/>


## ğŸ› ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/SyamSundarAkina/Fine_Tuning_our_own_dataset.git
cd Fine_Tuning_our_own_dataset
